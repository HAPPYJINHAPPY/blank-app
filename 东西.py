from volcenginesdkarkruntime import Ark
import pandas as pd
import pickle
from matplotlib import pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import streamlit as st
from matplotlib import font_manager
import os

font_path = "C:/Users/X2006936/Downloads/SourceHanSansCN-Normal.otf"  # æ›¿æ¢ä¸ºä½ çš„ä¸Šä¼ å­—ä½“æ–‡ä»¶å

# æ£€æŸ¥å­—ä½“æ–‡ä»¶æ˜¯å¦å­˜åœ¨
if not os.path.exists(font_path):
    st.error(f"Font file not found: {font_path}")
else:
    # è®¾ç½®å­—ä½“å±æ€§
    font_prop = font_manager.FontProperties(fname=font_path)
    font_name = font_prop.get_name()

    # åˆ›å»ºè‡ªå®šä¹‰å‡½æ•°æ¥ç»Ÿä¸€è®¾ç½®å­—ä½“
    def set_font_properties(ax, font_prop):
        """ç»Ÿä¸€è®¾ç½®åæ ‡è½´å’Œæ ‡é¢˜å­—ä½“"""
        for label in ax.get_xticklabels() + ax.get_yticklabels():
            label.set_fontproperties(font_prop)
        ax.title.set_fontproperties(font_prop)
        ax.xaxis.label.set_fontproperties(font_prop)
        ax.yaxis.label.set_fontproperties(font_prop)
    # å…¨å±€è®¾ç½®å­—ä½“
    plt.rcParams['font.sans-serif'] = [font_name]
    plt.rcParams['axes.unicode_minus'] = False

# Load the uploaded file
file_path = 'corrected_fatigue_simulation_data.csv'
data = pd.read_csv(file_path, encoding='gbk')

# 1. Features and labels
X = data.drop(columns=["ç–²åŠ³ç­‰çº§"])
y = data["ç–²åŠ³ç­‰çº§"]

# Normalize column names to avoid spaces
X.columns = X.columns.str.replace(' ', '_')

# 2. Data split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 3. Model training
model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)

# 4. Predictions
y_pred = model.predict(X_test)

# 5. Evaluation
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
report = classification_report(y_test, y_pred)

# Save model
with open("fatigue_model.pkl", "wb") as f:
    pickle.dump(model, f)

@st.cache_resource
def load_model():
    with open("fatigue_model.pkl", "rb") as f:
        model = pickle.load(f)
    return model


model = load_model()

# åˆ›å»º Ark å®¢æˆ·ç«¯
API_KEY = st.text_input("è¯·è¾“å…¥ OpenAI API å¯†é’¥", type="password")
if not API_KEY:
    st.info("è¯·è¾“å…¥ OpenAI API å¯†é’¥ä»¥ç»§ç»­ã€‚", icon="ğŸ—ï¸")
else:
    client = Ark(api_key=API_KEY)

    # æ˜¾ç¤ºæ ‡é¢˜å’Œä»‹ç»
    st.title("ç–²åŠ³è¯„ä¼°ä¸ AI èŠå¤©åŠ©æ‰‹")
    st.write(
        "è¿™ä¸ªåº”ç”¨ç»“åˆäº†ç–²åŠ³è¯„ä¼°æ¨¡å‹å’Œ AI èŠå¤©æœºå™¨äººï¼Œæä¾›åŸºäºç”¨æˆ·è¾“å…¥çš„ç–²åŠ³è¯„ä¼°ç»“æœå’Œå®æ—¶å»ºè®®ã€‚"
    )

    # åˆå§‹åŒ–å­˜å‚¨æ‰€æœ‰é¢„æµ‹è®°å½•çš„åˆ—è¡¨
    if 'predictions' not in st.session_state:
        st.session_state.predictions = []

    # è¾“å…¥ç–²åŠ³è¯„ä¼°å‚æ•°
    col1, col2 = st.columns(2)

    with col1:
        neck_flexion = st.slider("é¢ˆéƒ¨å‰å±ˆ", 0, 60, 20)
        neck_extension = st.slider("é¢ˆéƒ¨åä»°", 0, 60, 25)
        shoulder_elevation = st.slider("è‚©éƒ¨ä¸Šä¸¾èŒƒå›´", 0, 180, 60)
        shoulder_forward = st.slider("è‚©éƒ¨å‰ä¼¸èŒƒå›´", 0, 90, 30)

    with col2:
        elbow_flexion = st.slider("è‚˜éƒ¨å±ˆä¼¸", 0, 180, 90)
        wrist_extension = st.slider("æ‰‹è…•èƒŒä¼¸", 0, 90, 15)
        wrist_deviation = st.slider("æ‰‹è…•æ¡¡å/å°ºå", 0, 45, 10)
        back_flexion = st.slider("èƒŒéƒ¨å±ˆæ›²èŒƒå›´", 0, 90, 20)

    # è¾“å…¥ä»»åŠ¡å‚æ•°
    st.subheader("æŒç»­æ—¶é—´")
    task_duration = st.number_input("æŒç»­æ—¶é—´ï¼ˆç§’ï¼‰", min_value=0, value=6)
    movement_frequency = st.number_input("é‡å¤é¢‘ç‡ï¼ˆæ¯åˆ†é’Ÿï¼‰", min_value=0, value=5)

    # æ±‡æ€»è¾“å…¥æ•°æ®
    input_data = pd.DataFrame({
        "é¢ˆéƒ¨å‰å±ˆ": [neck_flexion],
        "é¢ˆéƒ¨åä»°": [neck_extension],
        "è‚©éƒ¨ä¸Šä¸¾èŒƒå›´": [shoulder_elevation],
        "è‚©éƒ¨å‰ä¼¸èŒƒå›´": [shoulder_forward],
        "è‚˜éƒ¨å±ˆä¼¸": [elbow_flexion],
        "æ‰‹è…•èƒŒä¼¸": [wrist_extension],
        "æ‰‹è…•æ¡¡å/å°ºå": [wrist_deviation],
        "èƒŒéƒ¨å±ˆæ›²èŒƒå›´": [back_flexion],
        "æŒç»­æ—¶é—´": [task_duration],
        "é‡å¤é¢‘ç‡": [movement_frequency],
    })

    st.subheader("è¾“å…¥å‚æ•°")
    st.write(input_data)

    # è¯„ä¼°æŒ‰é’®
    result = None  # ç¡®ä¿å˜é‡ result åˆå§‹åŒ–
    if st.button("è¯„ä¼°"):
        with st.spinner("æ­£åœ¨è¯„ä¼°ï¼Œè¯·ç¨ç­‰..."):
            # æ¨¡å‹é¢„æµ‹
            prediction = model.predict(input_data)
            result = ["ä½ç–²åŠ³çŠ¶æ€", "ä¸­ç–²åŠ³çŠ¶æ€", "é«˜ç–²åŠ³çŠ¶æ€"][prediction[0]]
            st.success(f"è¯„ä¼°ç»“æœï¼š{result}")

            # ä¿å­˜è¯„ä¼°è®°å½•
            record = input_data.copy()
            record["è¯„ä¼°"] = result
            if 'predictions' not in st.session_state:
                st.session_state.predictions = []
            st.session_state.predictions.append(record)

    # å³ä¾§ç©ºç™½åŒºåŸŸæ‰©å±•ä¸º AI åˆ†æåŠ©æ‰‹
    if result is not None:
        # åˆ›å»ºä¸€ä¸ªé¢å¤–çš„å³ä¾§å¸ƒå±€
        with st.container():
            st.subheader("AI æ™ºèƒ½è¯„ä¼°åŠ©æ‰‹")
            if "messages" not in st.session_state:
                st.session_state.messages = [
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªç–²åŠ³è¯„ä¼°åŠ©æ‰‹ï¼ŒåŸºäºç”¨æˆ·çš„ç–²åŠ³çŠ¶æ€å’Œè§’åº¦æ•°æ®æä¾›å»ºè®®ã€‚"}]

            # AI è¾“å…¥æ„é€ 
            ai_input = f"ç”¨æˆ·çš„ç–²åŠ³çŠ¶æ€æ˜¯ï¼š{result}ã€‚\n" \
                       f"ç”¨æˆ·æä¾›çš„è§’åº¦æ•°æ®ä¸ºï¼šé¢ˆéƒ¨å‰å±ˆ{neck_flexion}åº¦ï¼Œé¢ˆéƒ¨åä»°{neck_extension}åº¦ï¼Œ" \
                       f"è‚©éƒ¨ä¸Šä¸¾èŒƒå›´{shoulder_elevation}åº¦ï¼Œè‚©éƒ¨å‰ä¼¸èŒƒå›´{shoulder_forward}åº¦ï¼Œ" \
                       f"è‚˜éƒ¨å±ˆä¼¸{elbow_flexion}åº¦ï¼Œæ‰‹è…•èƒŒä¼¸{wrist_extension}åº¦ï¼Œ" \
                       f"æ‰‹è…•æ¡¡å/å°ºå{wrist_deviation}åº¦ï¼ŒèƒŒéƒ¨å±ˆæ›²èŒƒå›´{back_flexion}åº¦ã€‚\n" \
                       f"è¯·åŸºäºè¿™äº›æ•°æ®ç»™å‡ºç”¨æˆ·çš„æ½œåœ¨äººå› å±å®³åˆ†æåŠæ”¹å–„å»ºè®®ã€‚"

            st.session_state.messages.append({"role": "user", "content": ai_input})

            # æ˜¾ç¤ºç°æœ‰èŠå¤©è®°å½•
            for message in st.session_state.messages:
                with st.chat_message(message["role"]):
                    st.markdown(message["content"])


                def call_ark_api(messages):
                    try:
                        ark_messages = [{"role": msg["role"], "content": msg["content"]} for msg in messages]
                        completion = client.chat.completions.create(
                            model="ep-20241226165134-6lpqj",  # ä½¿ç”¨æ­£ç¡®çš„ Ark æ¨¡å‹ID
                            messages=ark_messages,
                            stream=True
                        )

                        response = ""
                        for chunk in completion:
                            delta_content = chunk.choices[0].delta.content if hasattr(chunk.choices[0].delta,
                                                                                      "content") else ""
                            yield delta_content
                    except Exception as e:
                        st.error(f"è°ƒç”¨ Ark API æ—¶å‡ºé”™ï¼š{e}")
                        yield f"Error: {e}"


                # åˆ›å»ºå ä½ç¬¦æ˜¾ç¤ºæœºå™¨äººå›ç­”
                response_placeholder = st.empty()
                response = ""
                for partial_response in call_ark_api(st.session_state.messages):
                    response += partial_response
                    response_placeholder.markdown(response)

                # å°† AI å›å¤ä¿å­˜åˆ°ä¼šè¯çŠ¶æ€
                st.session_state.messages.append({"role": "assistant", "content": response})
            else:
                st.warning("è¯·å…ˆç‚¹å‡»è¯„ä¼°æŒ‰é’®ç”Ÿæˆç»“æœåå†æŸ¥çœ‹åˆ†æã€‚")
